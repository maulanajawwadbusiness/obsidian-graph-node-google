# Arnvoid System Documentation

## 1. Introduction
**Arnvoid** is a conversational graph interface designed for deep reasoning over complex knowledgebases. It acts as a "thinking partner" that lives inside the user's obsidian graph, providing context-aware synthesis and exploration.

The core flow is the **Paper Essence Pipeline**:
`Document (PDF/MD/TXT) -> AI Paper Analyzer -> 5 Key Nodes + Node Knowledge (Title/Summary) -> Interactive Graph -> Contextual Chat`.

## 2. UI Surface Map & Ownership

The application layers, ordered by z-index (lowest to highest):

1.  **The Canvas (Graph substrate)**
    *   **Rule**: Never under-reacts. If a panel or overlay is active, the canvas underneath MUST NOT receive pointer/wheel events.
    *   Owned by: `PhysicsEngine`.

2.  **Top-Left Brand (`BrandLabel`) & Bottom-Center Title (`MapTitleBlock`)**
    *   Subtle UI markers tied to the current document state.
    *   `MapTitleBlock` shows "Peta Pengetahuan 2D" and the AI-inferred title.
    *   **Rule**: `pointer-events: none` ensures they never steal clicks from the graph.

3.  **Node Popups (Context)**
    *   Floating cards appearing on node interaction. Entry point for MiniChat.
    *   Owned by: `PopupStore`.

4.  **Mini Chat (Quick Context)**
    *   Lightweight chat window attached to Node Popups.
    *   **Brain**: `PopupStore` wires this to real AI context.
    *   **Handoff**: Graduates conversation to Full Chat while preserving Node Knowledge.

5.  **Full Chatbar (Deep Reasoning)**
    *   Right-side panel for long-form synthesis.
    *   **Ownership**: Consumes all interaction within its bounds.
    *   Owned by: `FullChatStore`.

6.  **Analysis Overlay (`AnalysisOverlay`)**
    *   **Highest Layer**. Dims the screen during AI parsing/analysis.
    *   **Shielding Rule**: Blocks all pointer, wheel, and touch events to prevent graph disturbance during critical AI operations.

## 3. AI Architecture

Arnvoid uses a unified AI layer (`src/ai/`) that abstracts provider details behind a strict interface.

### A. The Core: `LLMClient`
*   **Contract**: All features (Chat, Prefill, Analyzer) talk to the `LLMClient` interface (`generateText`, `generateTextStream`, `generateStructured`).
*   **Provider Agnostic**: The application logic does not know if it's talking to OpenAI, OpenRouter, or a local model.
*   **Factory**: `createLLMClient` in `src/ai/index.ts` determines the implementation based on config.

### B. Current & Future State
*   **Current Reality**: We primarily use `OpenAIClient` which leverages the **Responses API** (`v1/responses`) for text and streaming.
*   **Future Unification**: We are moving towards a "Single SDK" model where we use the official OpenAI SDK for *both* OpenAI and OpenRouter (by swapping `baseURL` to `https://openrouter.ai/api/v1`).
    *   *Why*: Reduces code drift and unifies parsing logic.
    *   *Status*: Architecture defined, refactor pending.

### C. Core Modules
*   **Paper Analyzer (`src/ai/paperAnalyzer.ts`)**: Distills documents into 1 Main Topic + 4 Detailed Points.
*   **FullChat AI (`src/fullchat/fullChatAi.ts`)**: Handles the reasoning panel and handoff prefill.
*   **MiniChat AI (`src/popup/PopupStore.tsx`)**: Directly integrated into the popup store for node-specific queries.

### D. Behavior Doctrine
*   **Mode Switch**: `VITE_AI_MODE='real'` vs `'mock'`.
*   **Abort Model**: Every AI loop uses an `AbortController`. Quick kill on navigation.
*   **Fake Streaming**: Client-side character ticking (15ms) used where backend streaming is unavailable or for UI effect.
*   **Fallback**: Timeouts (15s for FullChat, 2.5s for Prefill) trigger graceful mock fallbacks.

## 4. Context Doctrine

Intelligence is relative to context. We maintain three levels:

1.  **Node Knowledge**: A specific node's `sourceTitle` and `sourceSummary` generated by the analyzer. Lives in `node.meta`.
2.  **Document Context**: The full `documentText` and the metadata (inferred title) held in `DocumentStore`.
3.  **Handoff Context**: When moving from Mini -> Full, the `pendingContext` object preserves history + specific node knowledge so the reasoning is coherent.

## 5. Performance Doctrine (The Sacred 60)

*   **Hot Loops**: No network calls, no state updates inside the physics tick.
*   **Streaming**: Autosize and DOM updates for streaming text are throttled (~50ms) to prevent layout thrashing.
*   **Throttled Scroll**: `safeScrollToBottom` uses `requestAnimationFrame` and near-bottom detection to avoid jarring jumps.
*   **Tick Decoupling**: Physics ticks are capped at `targetTickHz` (default 60) and do not scale with monitor refresh rate.
*   **Adaptive Degradation**: The engine shifts into `stressed`/`emergency`/`fatal` modes based on N/E thresholds with hysteresis; expensive passes throttle instead of cliffing.
*   **Fatal-Mode Guardrail**: If N/E exceed the safe envelope, heavy passes are skipped and the sim stays responsive rather than attempting full n^2.

### A. Operating Envelope
*   **Intended**: Paper-essence graphs and small clusters (roughly N <= 250, E <= 1200).
*   **Stressed**: N >= 250 or E >= 1200; spacing frequency reduces.
*   **Emergency**: N >= 500 or E >= 2000; springs staggered, spacing reduced further.
*   **Fatal**: N >= 900 or E >= 3000; heavy passes disabled to avoid app melt.

### B. Telemetry & Logs
Enable `debugPerf: true` to get per-second metrics:
*   `[RenderPerf]` -> `ticksPerSecond`, `avgTickMs`, `p95TickMs`, `maxTickMs`, `ticksPerFrame`, `droppedMs`.
*   `[PhysicsPerf]` -> per-pass ms, `nodes`, `links`, `mode`, `allocs`, `topoDrop`, `topoDup`.
*   `[PhysicsMode]` -> mode transitions.
*   `[PhysicsTopology]` -> edge cap drops.
*   `[PhysicsFatal]` -> fatal mode active (once per second).

